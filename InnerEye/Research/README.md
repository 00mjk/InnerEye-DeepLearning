# Unsupervised Feature Learning with CPC

**Internship Project**: Mathias Perslev, Spring 2020

A MSR research-project, supervised by Ozan Oktay.

---

#### Project Overview
The CPC research project implements contrastive predictive coding for self-supervised learning of deep image encoder
models.

#### Supported Models

* Patch-based CPC
* Dual-view CPC using medical images and segmentations

#### About The Code
This code lives within `InnerEye-DeepLearning`, but defines a largely self-contained sub-package. All models defined 
within the CPC package derive from the PyTorch Lightning (PL) `LightningModule` class and relies on the PL training 
pipeline. All models are trained using a single entry script pointing it a PL system/module to 
execute. The available PL systems are implemented in `cpc/pl_systems`. See usage below for further details.

#### Usage
The main entry point into the CPC codebase is `cpc/bin/cpc.py`. Installing the `InnerEye-DeepLearning` package with
PIP adds this entry point as a console script. Invoke the script with the `--help` argument to see a list of available
PL modules:

```
>> cpc --help
usage: cpc [--init_yaml_config] [yaml_path] [--from_yaml_config] [yaml_path] [--aml] [--pl_system] [pl system] [pl system script args...]

InnerEye Research CPC entry script
----------------------------------
Available PyTorch lightning systems:
- cpc_stl10_train
- cpc_med_train
- cpc_stl10_classifier
- cpc_med_encode
- cpc_dual_view_med_train
- cpc_tri_view_med_train

optional arguments:
  -h, --help            show this help message and exit
  --init_yaml_config INIT_YAML_CONFIG
                        Init a default configuration file at the path, then
                        exists.
  --from_yaml_config FROM_YAML_CONFIG
                        A path to a yaml configuration file. If specified, all
                        command line arguments are read from this file.
  --no_vc_check         Launch without checking if repo is dirty (allows non-
                        commited changes)
  --aml                 Submit this job to AML instead of running locally.
  --pl_system {cpc_stl10_train,cpc_med_train,cpc_stl10_classifier,cpc_med_encode,cpc_dual_view_med_train,cpc_tri_view_med_train, med_dual_view_supervised}
                        Name of the lightning system to run.
```

#### PL Systems Overview

| PL System               | Description                                                                   |
|-------------------------|-------------------------------------------------------------------------------|
| cpc_stl10_train         | Runs patch-based CPC on the STl-10 dataset                                    |
| cpc_stl10_classifier    | Performs down-stream classification with a model as output by cpc_stl10_train |
| cpc_med_train           | Runs patch-based CPC on a medical image dataset                               |
| cpc_dual_view_med_train | Runs dual-view CPC on a medical image dataset                                 |
| cpc_tri_view_med_train  | Runs tri-view CPC on a medical image dataset                                  |
| cpc_med_encode          | Encode a medical image dataset with a CPC model (e.g. from cpc_med_train)     |

#### Running a PL system
All logs, model checkpoints etc. generated by a given PL system is stored within the current directory (in folders 
`./logs` and `/outputs`). The recommended workflow is thus to create a project directory for each launched experiment.

A PL system is selected with the `--pl_system` flag, e.g. in order to run the `cpc_dual_view_med_train` system:

```
>> cpc --pl_system cpc_dual_view_med_train --help
... displays all parameters that this moduel accepts
```

Each PL system accepts its own set of arguments (hyperparameters for running that system). A module may take a great 
number of input parameters. It is often easier to create a yaml config file for a given script and launch the module
against these parameters instead:

```
>> cpc --init config.yaml --pl_system cpc_dual_view_med_train
```

This creates a file at `config.yaml` storing all relevant parametes for the `cpc_dual_view_med_train` module. The config
is initialized with default parameters. Modify as fit for the given experiment, then run:

```
>> cpc --from config.yaml --pl_system cpc_dual_view_med_train
```


#### Encoding a dataset
After running the above code a project folder may look the following:

```
my_project_folder/
  │   config.yaml
  └───logs/
  │   │   log.txt
  │   │   tensorboard/
  │   │   ...
  └───outputs/
  │   │   classifier/
  │   │   plots/
  │   │   epoch=250-val_loss=0.1500.ckpt
  ...
```

A typical use-case would be to encode the full medical image dataset using one of the output models. The `cpc_med_encode` PL 
system does that using a model ckpt file as output by e.g. `cpc_med_train`, `cpc_dual_view_med_train`
or `cpc_tri_view_med_train`. Per default the dataset as described in the `config.yaml` when training the model will be
used, but the dataset may also be specified at run-time:

```
>> cpc --pl_system cpc_med_encode \
       --encodings_out_path epoch_250_w0_w1.pickle \
       --cpc_model_ckpt outputs/epoch=250-val_loss=0.1500.ckpt \
       --csv_file_name <some filename> \
       --hdf5_files_path <some path> \
```

The resulting file of pickled encodings `epoch_250_w0_w1.pickle` stores encodings and other variables on all subjects
from the specified dataset. This file may be input to a downstream classifier or visualization scripts 
(see e.g. `InnerEye/Research/cpc/bin/plot_encodings.py`).


#### Running in AML
Use the `--aml` flag to send a job for execution in AML:
```
>> cpc --from config.yaml --pl_system cpc_dual_view_med_train --aml
```
